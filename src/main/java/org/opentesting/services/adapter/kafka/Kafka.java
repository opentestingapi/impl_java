package org.opentesting.services.adapter.kafka;

import java.time.Instant;
import java.time.ZoneId;
import java.util.*;

import javax.annotation.PreDestroy;

import org.opentesting.dto.TestCaseCheckDTO;
import org.opentesting.dto.TestCaseDTO;
import org.opentesting.dto.TestCaseInjectionDTO;
import org.opentesting.dto.TestCaseServiceDTO;
import org.opentesting.dto.TestCaseValidationDTO;
import org.opentesting.services.adapter.Adapter;
import org.opentesting.services.pause.Pause;
import org.opentesting.services.prometheus.Prometheus;
import org.opentesting.util.LogExecutionTime;
import org.opentesting.util.exceptions.ConnectFailedException;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.scheduling.TaskScheduler;
import org.springframework.stereotype.Component;

import lombok.extern.slf4j.Slf4j;

@Component
@Slf4j
public class Kafka extends Adapter {    

    @Autowired
    private OpenTestingKafkaProducerFactory kafkaProducerFactory;

    @Autowired
    private OpenTestingKafkaConsumer kafkaConsumerFactory;

    @Autowired
    private Prometheus prometheus;

    @Autowired
    private KafkaTrace kafkaTrace;

    @Autowired
    private TaskScheduler taskScheduler;

    // producer cache
    private HashMap<String, Producer<String, String>> producers = new HashMap<>();    

    @Override
    public String getServicename() {
        return "kafka";
    }

    @Override
    @LogExecutionTime
    public boolean inject(String testid, TestCaseInjectionDTO inject) {

        Producer<String, String> producer = null;
        String connectstr = "";
        try {

            // need to concatenate broker and topic
            connectstr = inject.getService().getConnectstring() + "#"
                    + inject.getService().getCustom("topic").getValue();

            // header need replacement of connectuser, connectpassword, jwtuser, jwtpassword
            // and randomdata
            Map<String, String> headerMap = this.getHeaders(testid, inject.getService().getCustom("header").getValue(),
                    inject.getRandomdata(), inject.getService());

            // add jwt if required
            headerMap = this.addJwt(testid, inject.getService(), headerMap,
                    inject.getService().getCustom("kafkaheaderjwt").getValue(), "");

            // get producer
            producer = getProducer(testid, inject.getService(), connectstr);
            if (producer == null)
                return false;

            // random data replacements
            String content = this.getFileAndAddTestData(testid, inject.getSourcefile(), inject.getRandomdata());

            // create record and add records
            ProducerRecord<String, String> kafkarecord = new ProducerRecord<>(
                    inject.getService().getCustom("topic").getValue(), UUID.randomUUID().toString(), content);
            headerMap.entrySet().forEach(es -> kafkarecord.headers().add(es.getKey(), es.getValue().getBytes()));

            // add trace info
            kafkaTrace.generateTraceInfo(kafkarecord);

            // send synchronously with delay
            String stringdelay = inject.getService().getCustom("senddelay").getValue();
            if (stringdelay == null || stringdelay.isEmpty()) stringdelay = "0"; //default no delay

            //handle send delay
            if (stringdelay.equalsIgnoreCase("0")) {
                //send without delay
                producer.send(kafkarecord);
            } else {
                //send with delay
                log.info(testid + " " + inject.getInjectid() + " " + inject.getInstanceid() + ": kafka send delay: "+stringdelay);
                DelayedSender ds = new DelayedSender(testid, inject, producer, kafkarecord);
                taskScheduler.schedule(ds, Instant.now().plusMillis(Long.parseLong(stringdelay)));
            }

            // increment Prometheus
            prometheus.incrementExecution("kafkaproducer", connectstr);

            return true;
        } catch (Throwable e) {
            log.error(testid + " " + inject.getInjectid() + " " + inject.getInstanceid() + ": inject failed", e);
            if (producer != null) {
                producer.close();
                String key = this.createConnectionKey(testid, connectstr, inject.getService().getUsername(),
                        inject.getService().getPassword());
                this.addFailedConnector(key);
            }
            return false;
        }
    }

    @Override
    @LogExecutionTime
    public boolean check(String testid, TestCaseCheckDTO check, Object... args) {
        try {

            String content = (String) args[0];
            Long timestamp = (Long) args[1];

            // validate age
            log.debug("check creation: " + check.getStartts().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli());
            log.debug("message creation: " + timestamp);
            if (check.getStartts().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli() >= timestamp)
                return false;

            boolean retvalue = true;

            // check all validations, sort first
            for (TestCaseValidationDTO validation : sortValidations(check.getValidations())) {
                // do validation
                if (!validateResult(testid, check, validation, content, "Kafka " + timestamp)) {
                    retvalue = false;
                }
            }

            return retvalue;

        } catch (Throwable e) {
            log.error(testid + " " + check.getCheckid() + " " + check.getInstanceid() + ": check failed", e);
            return false;
        }
    }

    @LogExecutionTime
    private Producer<String, String> getProducer(String testid, TestCaseServiceDTO service, String connectstr)
            throws ConnectFailedException {

        // no reuse because of different password configurations
        String key = this.createConnectionKey(testid, connectstr, service.getUsername(), service.getPassword());
        Producer<String, String> prod = producers.get(key);

        // block ones
        if (this.isFailedConnector(key)) {
            log.warn(testid + ": Kafka connection blocked - please check credentials and upload testcase: "
                    + service.getUsername() + "@" + connectstr);
            return null;
        }

        try {
            // create new
            if (prod == null) {
                prod = kafkaProducerFactory.createProducer(service);
                log.info("KafkaProducer created: " + prod.toString());
                producers.put(key, prod);
            }

            return prod;
        } catch (Exception e) {
            String message = testid + ": Kafka connection failed - blocked: " + service.getUsername() + "@"
                    + connectstr;
            log.error(message, e);
            this.addFailedConnector(key);
            throw new ConnectFailedException(message);
        }
    }

    @PreDestroy
    @LogExecutionTime
    private void close() {
        // close consumers
        kafkaConsumerFactory.stopConsumers();
        // parallel close
        producers.values().parallelStream().forEach(prod -> {
            try {
                prod.close();
                log.info("KafkaProducer closed: " + prod.toString());
            } catch (Exception e) {
                log.warn("cannot close producer", e);
            }
        });
    }

    @Override
    @LogExecutionTime
    public void createRequiredComponents(TestCaseDTO test) {

        // create required Kafka consumer
        for (TestCaseCheckDTO check : test.getChecks()) {
            if (getAllServicenames().contains(check.getService().getType()) && check.isActive()) {
                kafkaConsumerFactory.closeConsumer(test.getId(), check.getService(), this);
                kafkaConsumerFactory.createConsumer(test.getId(), check.getService(), this);
            }
        }

        // remove blocked connections and existing producers
        this.removeFailedConnectorStartingWith(test.getId());
        HashMap<String, Producer<String, String>> newProducers = new HashMap<>();
        for (Map.Entry<String,Producer<String, String>> entry : producers.entrySet()) {
            if (entry.getKey().startsWith(test.getId())) {
                entry.getValue().close();
                log.info("closed: " + entry.getValue().toString());
            } else {
                newProducers.put(entry.getKey(), entry.getValue());
            }
        }
        producers = newProducers;
    }

    @Override
    @LogExecutionTime
    public List<String> getRequiredTimerCrons() {
        return Arrays.asList(openTestingConfig.getCheckcron());
    }

    /**
     * timer should validate age only
     */
    @Override
    public boolean timerValidateAgeOnly() {
        return true;
    }

    /**
     * pause method override to start and stop consumers
     */
    @Override
    @LogExecutionTime
    public void pause(boolean pause, String byLabel) {
        // we can stop consumers if ALLPAUSELABEL is used, otherwise consumers will stay
        // active as they anyways should use dedicated consumer group
        if (byLabel.equals(Pause.ALLPAUSELABEL)) {
            if (pause) {
                kafkaConsumerFactory.stopConsumers();
            } else {
                kafkaConsumerFactory.startConsumers();
            }
        }
    }

}
