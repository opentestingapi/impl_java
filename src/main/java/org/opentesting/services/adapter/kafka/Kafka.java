package org.opentesting.services.adapter.kafka;

import java.time.ZoneId;
import java.util.*;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;

import javax.annotation.PreDestroy;

import org.opentesting.services.adapter.Adapter;
import org.opentesting.services.execution.TestManager;
import org.opentesting.services.execution.dto.TestCaseCheckDTO;
import org.opentesting.services.execution.dto.TestCaseDTO;
import org.opentesting.services.execution.dto.TestCaseInjectionDTO;
import org.opentesting.services.execution.dto.TestCaseRandomDataDTO;
import org.opentesting.services.prometheus.Prometheus;
import org.opentesting.util.LogExecutionTime;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;

import lombok.extern.slf4j.Slf4j;

@Controller
@Slf4j
public class Kafka extends Adapter {    

    private static final String BROKER = "broker";
    private static final String TOPIC = "topic";

    @Autowired
    private OpenTestingKafkaProducerFactory kafkaProducerFactory;

    @Autowired
    private OpenTestingKafkaConsumerFactory kafkaConsumerFactory;

    @Autowired
    private Prometheus prometheus;

    //timer created flag
    private boolean timerCreated = false;    

    //producer cache
    private HashMap<String, KafkaProducer<String, String>> producers = new HashMap<>();

    @Override
    public String getServicename() {
        return "kafka";
    }

    @Override
    @LogExecutionTime
    public boolean inject(String testid, TestCaseInjectionDTO inject, TestCaseRandomDataDTO randomdata) {
        Map<String,String> connect = null;
        KafkaProducer<String, String> producer = null;
        try {

            //broker=kafka-broker:9092;topic=mytopicname
            connect = this.toParameterMap(inject.getConnectstring());

            //serviceparam need replacement of connectuser, connectpassword, jwtuser, jwtpassword
            Map<String, String> serviceparam =
                    this.replaceUserAndPassword(this.toParameterMap(inject.getServiceparam()), inject.getConnectuser(), inject.getConnectpassword(),
                            inject.getJwtuser(), inject.getJwtpassword());

            //header need replacement of connectuser, connectpassword, jwtuser, jwtpassword and randomdata
            Map<String, String> headerMap = this.getHeaders(testid, serviceparam.get("header"), randomdata,
                    inject.getConnectuser(), inject.getConnectpassword(), inject.getJwtuser(), inject.getJwtpassword());

            //add jwt if required
            headerMap = this.addJwt(testid, serviceparam, headerMap, serviceparam.get("kafka-header-jwt"), "");

            //get producer
            producer = getProducer(testid, connect.get(BROKER), inject.getConnectuser(),
                    inject.getConnectpassword(), inject.getServiceparam());
            if (producer == null) return false;

            //random data replacements
            String content = this.readFileAndAddTestData(testid, inject.getSourcefile(), randomdata);

            //create record and add records
            ProducerRecord<String, String> record = new ProducerRecord<String, String>(connect.get(TOPIC), UUID.randomUUID().toString(), content);
            headerMap.entrySet().forEach(
                    es -> {
                        record.headers().add(es.getKey(), es.getValue().getBytes());
                    }
            );

            //send synchronously
            Future<RecordMetadata> future = producer.send(record);
            future.get(30, TimeUnit.SECONDS);

            //increment Prometheus
            prometheus.incrementExecution("kafkaproducer", inject.getConnectstring());

            return true;
        } catch (InterruptedException ie) {
            log.error(testid+" "+inject.getInjectid()+": inject interrupted", ie);
            // Restore interrupted state...
            Thread.currentThread().interrupt();
            return false;
        } catch (Throwable e) {
            log.error(testid+" "+inject.getInjectid()+": inject failed", e);
            if (producer != null) {
                producer.close();
                String key = this.createConnectionKey(testid, connect.get(BROKER), inject.getConnectuser(),
                        inject.getConnectpassword());
                this.addFailedConnector(key);
            }
            return false;
        }
    }

    @Override
    @LogExecutionTime
    public boolean check(TestCaseCheckDTO check, Object... args) {
        try {
            String content = (String) args[0];
            Long timestamp = (Long) args[1];            

            //validate age            
            log.debug("check creation: "+check.getStartts().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli());
            log.debug("message creation: "+timestamp);
            if (check.getStartts().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli() >= timestamp) return false;

            //random data replacements
            String expected = this.readFileAndAddTestData(check.getTestid(), check.getExpectedfile(), check.getRandomdata());  
            log.debug(">"+content+"< >"+expected+"<");          

            //expected result check
            boolean retvalue = false;
            switch (check.getExpectedtype()) {
                case "contains":
                    if (content.contains(expected))
                        retvalue = true;
                    break;
                case "equals":
                    if (content.equals(expected))
                        retvalue = true;
                    break;
                default:
                    log.warn("unknown expected type: >" + check.getExpectedtype() + "<");
            }

            //check failed
            if (!retvalue) log.info("Kafka check result:\nTimestamp: "+timestamp+"\n>"+content+"<\n>"+expected+"<");

            //add result2random attributes
            check.setRandomdata(this.addCheckResult2Random(check.getTestid(), check, check.getRandomdata(), content));

            return retvalue;
        } catch (Throwable e) {
            log.error(check.getTestid()+" "+check.getCheckid()+": check failed", e);
            return false;
        }
    }

    @LogExecutionTime
    private KafkaProducer<String, String> getProducer(String testid, String broker, String connectuser, String connectpassword, String serviceparamstr) throws Exception {

        //no reuse because of different password configurations
        String key = this.createConnectionKey(testid, broker, connectuser, connectpassword);
        KafkaProducer<String, String> prod = producers.get(key);

        //block ones
        if (this.isFailedConnector(key)) {
            log.warn(testid + ": Kafka connection blocked - please check credentials and upload testcase: " + connectuser + "@" + broker);
            return null;
        }

        try {
            //create new
            if (prod == null) {
                Map<String,String> serviceparam = this.toParameterMap(serviceparamstr);
                prod = kafkaProducerFactory.createProducer(broker, connectuser, decryptPassword(connectpassword), serviceparam);
                log.info("KafkaProducer created: "+prod.toString());
                producers.put(key, prod);
            }

            return prod;
        } catch (Exception e) {
            String message = testid+": Kafka connection failed - blocked: "+connectuser+"@"+broker;
            log.error(message, e);
            this.addFailedConnector(key);
            throw new Exception(message);
        }
    }

    @PreDestroy
    @LogExecutionTime
    private void close() {
        //parallel close
        producers.values().parallelStream().forEach(
            prod -> {
                try {
                    prod.close();
                    log.info("KafkaProducer closed: "+prod.toString());
                } catch (Exception e) {
                    log.warn("cannot close producer", e);
                }
            }
        );              
    }

    @Override
    @LogExecutionTime
    public void createRequiredComponents(TestCaseDTO test) {

        //timer to check outdated max wait times
        if (!timerCreated && doChecksContainServices(test, getAllServicenames())) {
            testPlaner.createCheckTimer(getAllServicenames(), e2ETestConfig.getCheckcron());
            timerCreated = true;
            log.info("KafkaTimer created");
        }

        //create required Kafka consumer
        for (TestCaseCheckDTO check : test.getChecks()) {
            if (getAllServicenames().contains(check.getService()) && check.isActivation()) {
                 //broker=kafka-broker:9092,topic=mytopicname
                Map<String,String> connect = this.toParameterMap(check.getConnectstring());
                Map<String,String> serviceparam = this.toParameterMap(check.getServiceparam());
                kafkaConsumerFactory.createConsumer(check.getConnectstring(), connect.get(BROKER), connect.get(TOPIC),
                        check.getConnectuser(), decryptPassword(check.getConnectpassword()), serviceparam);
            }
        }

        //remove blocked connections and existing producers
        this.removeFailedConnectorStartingWith(test.getTestid());
        HashMap<String, KafkaProducer<String, String>> newProducers = new HashMap<String,KafkaProducer<String, String>>();
        for (Map.Entry<String,KafkaProducer<String, String>> entry : producers.entrySet()) {
            if (entry.getKey().startsWith(test.getTestid())) {
                entry.getValue().close();
                log.info("closed: "+entry.getValue().toString());
            } else {
                newProducers.put(entry.getKey(), entry.getValue());
            }
        }
        producers = newProducers;
    }    

    /**
     * pause method override to start and stop consumers
     */
    @Override
    @LogExecutionTime
    public void pause(boolean pause, String byLabel) {
        //we can stop consumers if ALLPAUSELABEL is used, otherwise consumers will stay active as they anyways should use dedicated consumer group
        if (byLabel.equals(TestManager.ALLPAUSELABEL)) {
            if (pause) {
                kafkaConsumerFactory.stopConsumers();
            } else {
                kafkaConsumerFactory.startConsumers();
            }
        }     
    }

}
